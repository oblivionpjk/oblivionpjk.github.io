---
title: 逻辑回归与朴素贝叶斯的比较
tags: 算法 algorithm 
categories: algorithm
---

这是论文《On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes》的一些笔记

# Abstract
逻辑回归和朴素贝叶斯就是判别和生成算法的代表。
人们通常喜欢使用判别分类算法，这源于在重复实验中证明的一个观察，即虽然判别学习具有较低的渐近误差，但生成分类器也可以更快地接近其（较高的）渐近误差。

# Introduction
Discriminative(判别）学习算法是一类模型化输入（X）输出（Y）的关系的方法。判别学习算法要求X的各个项是相互独立变化的。只需了解Ｘ对Ｙ的决定关系，即model Ｐ（Ｙ｜Ｘ）．学习的结果就是X参数所决定的Y参数。
Generative（生成）学习算法，是一类描述输出或中间参数（Y）产生输入参数（X）的概率的方法。需要的结果是其实也是model P(Y|X)，只不过是看X属于哪个Y几率更大就属于哪个Y。但这里不直接使用P（Y | X），因为这可能非常复杂，或者统计出来的结果没有明显意义，例如model 在give 一个X=吸烟的人，Y得肺癌的几率。而是model P(X|Y)，就好像在得肺癌的人里面看吸烟人的情况。我们都知道吸烟的人数亿亿计，但得肺癌的人逼近是少数，model 从得肺癌的人出发，更加容易，比如10万个得肺癌的，抽样1000个就差不多了。 生成学习算法考察X的每个项对Y的概率分布情况，即P(Y|{x1,x2,x3...xn})，需要考察P(x1|Y)...P(xn|Y)......
人们使用判别算法而不是生成算法的一大原因由Vapnik说的“分类问题应该被直接的解决同时不应该产生新的问题作为中间步骤（比如建模P(x|y))”
论文的目的：
（a） 生成模型确实有更高的渐进误差（随着训练实例的数量变大）但是（b）生成模型也可能会更快的到达其渐进错误相比于判别模型-可能有许多训练实例参数的数目是对数的，而不是线性的

# Preliminaries




参考链接：https://blog.csdn.net/dllian/article/details/7492325
